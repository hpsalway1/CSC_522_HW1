{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df76861e6b3d5417",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 1\n",
    "\n",
    "In this homework you will be doing Decision Tree classification on the Breast Cancer dataset.\n",
    "We will also be using data preprocessing to test ways to improve the classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-445df16c3a9c5382",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b92d511365b5816",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# you should be familiar with numpy, pandas and matplotlib from HW0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# we're using the Breast Cancer dataset from sklearn.datasets\n",
    "from sklearn import datasets\n",
    "\n",
    "# we will also be using the PCA library from scikit learn for this exercise\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# we will use the StandardScaler method to z-score normalize our data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Remember you have to run this cell block before continuing!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb4995075486ce2e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "breast_cancer_sk = datasets.load_breast_cancer(as_frame=True)\n",
    "breast_cancer = pd.DataFrame(breast_cancer_sk.data, columns = breast_cancer_sk.feature_names)\n",
    "breast_cancer[\"target\"] = breast_cancer_sk.target\n",
    "breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c30171bad3bafef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Random Sampling [Anurata Hridi]\n",
    "\n",
    "In this following exercise, you will be writting code to implement random sampling without replacement from scratch. No additional libraries are allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random is the only additional library you can use for this problem\n",
    "import random\n",
    "\n",
    "def random_sampling(data, n):\n",
    "    \"\"\"\n",
    "    Input: data: the pandas dataframe to sample.\n",
    "           n: the number of samples (rows) to take\n",
    "    Output: The randomly sampled dataset (without replacement) as a pd.Dataframe.\n",
    "    Hint: You should look up the random.shuffle function\n",
    "    \"\"\"\n",
    "    #Get indices of the DataFrame\n",
    "    indices = list(data.index)\n",
    "\n",
    "    #shuffle the indices using random.shuffle to get a random order\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    #select the first 'n' indices\n",
    "    selected_indices = indices[:n]\n",
    "\n",
    "    #Return the sampled DataFrame\n",
    "    return data.loc[selected_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function\n",
    "sample = random_sampling(breast_cancer,30)                                                                                                                                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at the distribution of the species (target attribute)\n",
    "# How evenly are the species distributed with random sampling?\n",
    "plt.hist(sample[\"target\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try running it again - are the results the same?\n",
    "plt.hist(sample[\"target\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that the correct number of rows were samples\n",
    "np.testing.assert_equal(sample.shape,(30, 31))\n",
    "# Assert sampling was done without replacement\n",
    "assert sample.drop_duplicates().shape[0] == 30\n",
    "# Assert that the first row is present in the original dataframe\n",
    "assert any([(breast_cancer.iloc[i,:] == sample.iloc[0,:]).all() for i in breast_cancer.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d63a83e3a1c6f263",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Stratified sampling [Anurata Hridi]\n",
    "\n",
    "In this part, you will be writing code to do stratified sampling. Create a stratrified sample of the `breast_cancer` dataset, with 40 objects, that has an equal number of each **target** value (0 and 1).\n",
    "\n",
    "**Store it in the variable `stratified_breast_cancer`**.\n",
    "\n",
    "**Hint**: You should read about the [split-apply-combine](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html) coding pattern in Pandas before starting this problem! In particular pay attention to the following:\n",
    "* [Splitting an object into groups](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups)\n",
    "* [Transformation](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#transformation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sampling(data, n, attr):\n",
    "    \"\"\"\n",
    "    Input: data: the dataset to sample\n",
    "           n: the number of instances sampled from each value of the given attribute\n",
    "           attr: the attribute to stratify on\n",
    "    Output: The sampled dataset in pd.Dataframe format\n",
    "    \n",
    "    Allowed functions: df.groupby, df.apply, df.sample\n",
    "    Hint: See the link in the function description above.\n",
    "    \"\"\"\n",
    "    #Group the data by the speified attribute\n",
    "    grouped = data.groupby(attr)\n",
    "\n",
    "    #Sample n items from each group\n",
    "    stratified_sample=grouped.apply(lambda x: x.sample(n=min(len(x), n))).reset_index(drop=True)\n",
    "\n",
    "    return stratified_sample\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.920</td>\n",
       "      <td>25.09</td>\n",
       "      <td>143.00</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.22360</td>\n",
       "      <td>0.31740</td>\n",
       "      <td>0.14740</td>\n",
       "      <td>0.2149</td>\n",
       "      <td>0.06879</td>\n",
       "      <td>...</td>\n",
       "      <td>29.41</td>\n",
       "      <td>179.10</td>\n",
       "      <td>1819.0</td>\n",
       "      <td>0.14070</td>\n",
       "      <td>0.41860</td>\n",
       "      <td>0.65990</td>\n",
       "      <td>0.25420</td>\n",
       "      <td>0.2929</td>\n",
       "      <td>0.09873</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.120</td>\n",
       "      <td>16.68</td>\n",
       "      <td>98.78</td>\n",
       "      <td>716.6</td>\n",
       "      <td>0.08876</td>\n",
       "      <td>0.09588</td>\n",
       "      <td>0.07550</td>\n",
       "      <td>0.04079</td>\n",
       "      <td>0.1594</td>\n",
       "      <td>0.05986</td>\n",
       "      <td>...</td>\n",
       "      <td>20.24</td>\n",
       "      <td>117.70</td>\n",
       "      <td>989.5</td>\n",
       "      <td>0.14910</td>\n",
       "      <td>0.33310</td>\n",
       "      <td>0.33270</td>\n",
       "      <td>0.12520</td>\n",
       "      <td>0.3415</td>\n",
       "      <td>0.09740</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.770</td>\n",
       "      <td>21.43</td>\n",
       "      <td>122.90</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>0.09116</td>\n",
       "      <td>0.14020</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.06090</td>\n",
       "      <td>0.1953</td>\n",
       "      <td>0.06083</td>\n",
       "      <td>...</td>\n",
       "      <td>34.37</td>\n",
       "      <td>161.10</td>\n",
       "      <td>1873.0</td>\n",
       "      <td>0.14980</td>\n",
       "      <td>0.48270</td>\n",
       "      <td>0.46340</td>\n",
       "      <td>0.20480</td>\n",
       "      <td>0.3679</td>\n",
       "      <td>0.09870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.050</td>\n",
       "      <td>19.08</td>\n",
       "      <td>113.40</td>\n",
       "      <td>895.0</td>\n",
       "      <td>0.11410</td>\n",
       "      <td>0.15720</td>\n",
       "      <td>0.19100</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.06325</td>\n",
       "      <td>...</td>\n",
       "      <td>24.89</td>\n",
       "      <td>133.50</td>\n",
       "      <td>1189.0</td>\n",
       "      <td>0.17030</td>\n",
       "      <td>0.39340</td>\n",
       "      <td>0.50180</td>\n",
       "      <td>0.25430</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.09061</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.550</td>\n",
       "      <td>28.77</td>\n",
       "      <td>133.60</td>\n",
       "      <td>1207.0</td>\n",
       "      <td>0.09260</td>\n",
       "      <td>0.20630</td>\n",
       "      <td>0.17840</td>\n",
       "      <td>0.11440</td>\n",
       "      <td>0.1893</td>\n",
       "      <td>0.06232</td>\n",
       "      <td>...</td>\n",
       "      <td>36.27</td>\n",
       "      <td>178.60</td>\n",
       "      <td>1926.0</td>\n",
       "      <td>0.12810</td>\n",
       "      <td>0.53290</td>\n",
       "      <td>0.42510</td>\n",
       "      <td>0.19410</td>\n",
       "      <td>0.2818</td>\n",
       "      <td>0.10050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.480</td>\n",
       "      <td>21.46</td>\n",
       "      <td>94.25</td>\n",
       "      <td>648.2</td>\n",
       "      <td>0.09444</td>\n",
       "      <td>0.09947</td>\n",
       "      <td>0.12040</td>\n",
       "      <td>0.04938</td>\n",
       "      <td>0.2075</td>\n",
       "      <td>0.05636</td>\n",
       "      <td>...</td>\n",
       "      <td>29.25</td>\n",
       "      <td>108.40</td>\n",
       "      <td>808.9</td>\n",
       "      <td>0.13060</td>\n",
       "      <td>0.19760</td>\n",
       "      <td>0.33490</td>\n",
       "      <td>0.12250</td>\n",
       "      <td>0.3020</td>\n",
       "      <td>0.06846</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15.660</td>\n",
       "      <td>23.20</td>\n",
       "      <td>110.20</td>\n",
       "      <td>773.5</td>\n",
       "      <td>0.11090</td>\n",
       "      <td>0.31140</td>\n",
       "      <td>0.31760</td>\n",
       "      <td>0.13770</td>\n",
       "      <td>0.2495</td>\n",
       "      <td>0.08104</td>\n",
       "      <td>...</td>\n",
       "      <td>31.64</td>\n",
       "      <td>143.70</td>\n",
       "      <td>1226.0</td>\n",
       "      <td>0.15040</td>\n",
       "      <td>0.51720</td>\n",
       "      <td>0.61810</td>\n",
       "      <td>0.24620</td>\n",
       "      <td>0.3277</td>\n",
       "      <td>0.10190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16.020</td>\n",
       "      <td>23.24</td>\n",
       "      <td>102.70</td>\n",
       "      <td>797.8</td>\n",
       "      <td>0.08206</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>0.03299</td>\n",
       "      <td>0.03323</td>\n",
       "      <td>0.1528</td>\n",
       "      <td>0.05697</td>\n",
       "      <td>...</td>\n",
       "      <td>33.88</td>\n",
       "      <td>123.80</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.11810</td>\n",
       "      <td>0.15510</td>\n",
       "      <td>0.14590</td>\n",
       "      <td>0.09975</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.08452</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.480</td>\n",
       "      <td>20.82</td>\n",
       "      <td>88.40</td>\n",
       "      <td>559.2</td>\n",
       "      <td>0.10160</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.10630</td>\n",
       "      <td>0.05439</td>\n",
       "      <td>0.1720</td>\n",
       "      <td>0.06419</td>\n",
       "      <td>...</td>\n",
       "      <td>26.02</td>\n",
       "      <td>107.30</td>\n",
       "      <td>740.4</td>\n",
       "      <td>0.16100</td>\n",
       "      <td>0.42250</td>\n",
       "      <td>0.50300</td>\n",
       "      <td>0.22580</td>\n",
       "      <td>0.2807</td>\n",
       "      <td>0.10710</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.420</td>\n",
       "      <td>19.77</td>\n",
       "      <td>94.48</td>\n",
       "      <td>642.5</td>\n",
       "      <td>0.09752</td>\n",
       "      <td>0.11410</td>\n",
       "      <td>0.09388</td>\n",
       "      <td>0.05839</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>0.06390</td>\n",
       "      <td>...</td>\n",
       "      <td>30.86</td>\n",
       "      <td>109.50</td>\n",
       "      <td>826.4</td>\n",
       "      <td>0.14310</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.31940</td>\n",
       "      <td>0.15650</td>\n",
       "      <td>0.2718</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13.030</td>\n",
       "      <td>18.42</td>\n",
       "      <td>82.61</td>\n",
       "      <td>523.8</td>\n",
       "      <td>0.08983</td>\n",
       "      <td>0.03766</td>\n",
       "      <td>0.02562</td>\n",
       "      <td>0.02923</td>\n",
       "      <td>0.1467</td>\n",
       "      <td>0.05863</td>\n",
       "      <td>...</td>\n",
       "      <td>22.81</td>\n",
       "      <td>84.46</td>\n",
       "      <td>545.9</td>\n",
       "      <td>0.09701</td>\n",
       "      <td>0.04619</td>\n",
       "      <td>0.04833</td>\n",
       "      <td>0.05013</td>\n",
       "      <td>0.1987</td>\n",
       "      <td>0.06169</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9.465</td>\n",
       "      <td>21.01</td>\n",
       "      <td>60.11</td>\n",
       "      <td>269.4</td>\n",
       "      <td>0.10440</td>\n",
       "      <td>0.07773</td>\n",
       "      <td>0.02172</td>\n",
       "      <td>0.01504</td>\n",
       "      <td>0.1717</td>\n",
       "      <td>0.06899</td>\n",
       "      <td>...</td>\n",
       "      <td>31.56</td>\n",
       "      <td>67.03</td>\n",
       "      <td>330.7</td>\n",
       "      <td>0.15480</td>\n",
       "      <td>0.16640</td>\n",
       "      <td>0.09412</td>\n",
       "      <td>0.06517</td>\n",
       "      <td>0.2878</td>\n",
       "      <td>0.09211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.910</td>\n",
       "      <td>16.33</td>\n",
       "      <td>82.53</td>\n",
       "      <td>516.4</td>\n",
       "      <td>0.07941</td>\n",
       "      <td>0.05366</td>\n",
       "      <td>0.03873</td>\n",
       "      <td>0.02377</td>\n",
       "      <td>0.1829</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>22.00</td>\n",
       "      <td>90.81</td>\n",
       "      <td>600.6</td>\n",
       "      <td>0.10970</td>\n",
       "      <td>0.15060</td>\n",
       "      <td>0.17640</td>\n",
       "      <td>0.08235</td>\n",
       "      <td>0.3024</td>\n",
       "      <td>0.06949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.480</td>\n",
       "      <td>14.98</td>\n",
       "      <td>67.49</td>\n",
       "      <td>333.6</td>\n",
       "      <td>0.09816</td>\n",
       "      <td>0.10130</td>\n",
       "      <td>0.06335</td>\n",
       "      <td>0.02218</td>\n",
       "      <td>0.1925</td>\n",
       "      <td>0.06915</td>\n",
       "      <td>...</td>\n",
       "      <td>21.57</td>\n",
       "      <td>81.41</td>\n",
       "      <td>440.4</td>\n",
       "      <td>0.13270</td>\n",
       "      <td>0.29960</td>\n",
       "      <td>0.29390</td>\n",
       "      <td>0.09310</td>\n",
       "      <td>0.3020</td>\n",
       "      <td>0.09646</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12.180</td>\n",
       "      <td>20.52</td>\n",
       "      <td>77.22</td>\n",
       "      <td>458.7</td>\n",
       "      <td>0.08013</td>\n",
       "      <td>0.04038</td>\n",
       "      <td>0.02383</td>\n",
       "      <td>0.01770</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.05677</td>\n",
       "      <td>...</td>\n",
       "      <td>32.84</td>\n",
       "      <td>84.58</td>\n",
       "      <td>547.8</td>\n",
       "      <td>0.11230</td>\n",
       "      <td>0.08862</td>\n",
       "      <td>0.11450</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.2694</td>\n",
       "      <td>0.06878</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11.840</td>\n",
       "      <td>18.94</td>\n",
       "      <td>75.51</td>\n",
       "      <td>428.0</td>\n",
       "      <td>0.08871</td>\n",
       "      <td>0.06900</td>\n",
       "      <td>0.02669</td>\n",
       "      <td>0.01393</td>\n",
       "      <td>0.1533</td>\n",
       "      <td>0.06057</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>85.22</td>\n",
       "      <td>546.3</td>\n",
       "      <td>0.12800</td>\n",
       "      <td>0.18800</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.06913</td>\n",
       "      <td>0.2535</td>\n",
       "      <td>0.07993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12.960</td>\n",
       "      <td>18.29</td>\n",
       "      <td>84.18</td>\n",
       "      <td>525.2</td>\n",
       "      <td>0.07351</td>\n",
       "      <td>0.07899</td>\n",
       "      <td>0.04057</td>\n",
       "      <td>0.01883</td>\n",
       "      <td>0.1874</td>\n",
       "      <td>0.05899</td>\n",
       "      <td>...</td>\n",
       "      <td>24.61</td>\n",
       "      <td>96.31</td>\n",
       "      <td>621.9</td>\n",
       "      <td>0.09329</td>\n",
       "      <td>0.23180</td>\n",
       "      <td>0.16040</td>\n",
       "      <td>0.06608</td>\n",
       "      <td>0.3207</td>\n",
       "      <td>0.07247</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9.876</td>\n",
       "      <td>19.40</td>\n",
       "      <td>63.95</td>\n",
       "      <td>298.3</td>\n",
       "      <td>0.10050</td>\n",
       "      <td>0.09697</td>\n",
       "      <td>0.06154</td>\n",
       "      <td>0.03029</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.06322</td>\n",
       "      <td>...</td>\n",
       "      <td>26.83</td>\n",
       "      <td>72.22</td>\n",
       "      <td>361.2</td>\n",
       "      <td>0.15590</td>\n",
       "      <td>0.23020</td>\n",
       "      <td>0.26440</td>\n",
       "      <td>0.09749</td>\n",
       "      <td>0.2622</td>\n",
       "      <td>0.08490</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11.040</td>\n",
       "      <td>14.93</td>\n",
       "      <td>70.67</td>\n",
       "      <td>372.7</td>\n",
       "      <td>0.07987</td>\n",
       "      <td>0.07079</td>\n",
       "      <td>0.03546</td>\n",
       "      <td>0.02074</td>\n",
       "      <td>0.2003</td>\n",
       "      <td>0.06246</td>\n",
       "      <td>...</td>\n",
       "      <td>20.83</td>\n",
       "      <td>79.73</td>\n",
       "      <td>447.1</td>\n",
       "      <td>0.10950</td>\n",
       "      <td>0.19820</td>\n",
       "      <td>0.15530</td>\n",
       "      <td>0.06754</td>\n",
       "      <td>0.3202</td>\n",
       "      <td>0.07287</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.940</td>\n",
       "      <td>18.59</td>\n",
       "      <td>70.39</td>\n",
       "      <td>370.0</td>\n",
       "      <td>0.10040</td>\n",
       "      <td>0.07460</td>\n",
       "      <td>0.04944</td>\n",
       "      <td>0.02932</td>\n",
       "      <td>0.1486</td>\n",
       "      <td>0.06615</td>\n",
       "      <td>...</td>\n",
       "      <td>25.58</td>\n",
       "      <td>82.76</td>\n",
       "      <td>472.4</td>\n",
       "      <td>0.13630</td>\n",
       "      <td>0.16440</td>\n",
       "      <td>0.14120</td>\n",
       "      <td>0.07887</td>\n",
       "      <td>0.2251</td>\n",
       "      <td>0.07732</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        20.920         25.09          143.00     1347.0          0.10990   \n",
       "1        15.120         16.68           98.78      716.6          0.08876   \n",
       "2        18.770         21.43          122.90     1092.0          0.09116   \n",
       "3        17.050         19.08          113.40      895.0          0.11410   \n",
       "4        19.550         28.77          133.60     1207.0          0.09260   \n",
       "5        14.480         21.46           94.25      648.2          0.09444   \n",
       "6        15.660         23.20          110.20      773.5          0.11090   \n",
       "7        16.020         23.24          102.70      797.8          0.08206   \n",
       "8        13.480         20.82           88.40      559.2          0.10160   \n",
       "9        14.420         19.77           94.48      642.5          0.09752   \n",
       "10       13.030         18.42           82.61      523.8          0.08983   \n",
       "11        9.465         21.01           60.11      269.4          0.10440   \n",
       "12       12.910         16.33           82.53      516.4          0.07941   \n",
       "13       10.480         14.98           67.49      333.6          0.09816   \n",
       "14       12.180         20.52           77.22      458.7          0.08013   \n",
       "15       11.840         18.94           75.51      428.0          0.08871   \n",
       "16       12.960         18.29           84.18      525.2          0.07351   \n",
       "17        9.876         19.40           63.95      298.3          0.10050   \n",
       "18       11.040         14.93           70.67      372.7          0.07987   \n",
       "19       10.940         18.59           70.39      370.0          0.10040   \n",
       "\n",
       "    mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0            0.22360         0.31740              0.14740         0.2149   \n",
       "1            0.09588         0.07550              0.04079         0.1594   \n",
       "2            0.14020         0.10600              0.06090         0.1953   \n",
       "3            0.15720         0.19100              0.10900         0.2131   \n",
       "4            0.20630         0.17840              0.11440         0.1893   \n",
       "5            0.09947         0.12040              0.04938         0.2075   \n",
       "6            0.31140         0.31760              0.13770         0.2495   \n",
       "7            0.06669         0.03299              0.03323         0.1528   \n",
       "8            0.12550         0.10630              0.05439         0.1720   \n",
       "9            0.11410         0.09388              0.05839         0.1879   \n",
       "10           0.03766         0.02562              0.02923         0.1467   \n",
       "11           0.07773         0.02172              0.01504         0.1717   \n",
       "12           0.05366         0.03873              0.02377         0.1829   \n",
       "13           0.10130         0.06335              0.02218         0.1925   \n",
       "14           0.04038         0.02383              0.01770         0.1739   \n",
       "15           0.06900         0.02669              0.01393         0.1533   \n",
       "16           0.07899         0.04057              0.01883         0.1874   \n",
       "17           0.09697         0.06154              0.03029         0.1945   \n",
       "18           0.07079         0.03546              0.02074         0.2003   \n",
       "19           0.07460         0.04944              0.02932         0.1486   \n",
       "\n",
       "    mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                  0.06879  ...          29.41           179.10      1819.0   \n",
       "1                  0.05986  ...          20.24           117.70       989.5   \n",
       "2                  0.06083  ...          34.37           161.10      1873.0   \n",
       "3                  0.06325  ...          24.89           133.50      1189.0   \n",
       "4                  0.06232  ...          36.27           178.60      1926.0   \n",
       "5                  0.05636  ...          29.25           108.40       808.9   \n",
       "6                  0.08104  ...          31.64           143.70      1226.0   \n",
       "7                  0.05697  ...          33.88           123.80      1150.0   \n",
       "8                  0.06419  ...          26.02           107.30       740.4   \n",
       "9                  0.06390  ...          30.86           109.50       826.4   \n",
       "10                 0.05863  ...          22.81            84.46       545.9   \n",
       "11                 0.06899  ...          31.56            67.03       330.7   \n",
       "12                 0.05667  ...          22.00            90.81       600.6   \n",
       "13                 0.06915  ...          21.57            81.41       440.4   \n",
       "14                 0.05677  ...          32.84            84.58       547.8   \n",
       "15                 0.06057  ...          24.99            85.22       546.3   \n",
       "16                 0.05899  ...          24.61            96.31       621.9   \n",
       "17                 0.06322  ...          26.83            72.22       361.2   \n",
       "18                 0.06246  ...          20.83            79.73       447.1   \n",
       "19                 0.06615  ...          25.58            82.76       472.4   \n",
       "\n",
       "    worst smoothness  worst compactness  worst concavity  \\\n",
       "0            0.14070            0.41860          0.65990   \n",
       "1            0.14910            0.33310          0.33270   \n",
       "2            0.14980            0.48270          0.46340   \n",
       "3            0.17030            0.39340          0.50180   \n",
       "4            0.12810            0.53290          0.42510   \n",
       "5            0.13060            0.19760          0.33490   \n",
       "6            0.15040            0.51720          0.61810   \n",
       "7            0.11810            0.15510          0.14590   \n",
       "8            0.16100            0.42250          0.50300   \n",
       "9            0.14310            0.30260          0.31940   \n",
       "10           0.09701            0.04619          0.04833   \n",
       "11           0.15480            0.16640          0.09412   \n",
       "12           0.10970            0.15060          0.17640   \n",
       "13           0.13270            0.29960          0.29390   \n",
       "14           0.11230            0.08862          0.11450   \n",
       "15           0.12800            0.18800          0.14710   \n",
       "16           0.09329            0.23180          0.16040   \n",
       "17           0.15590            0.23020          0.26440   \n",
       "18           0.10950            0.19820          0.15530   \n",
       "19           0.13630            0.16440          0.14120   \n",
       "\n",
       "    worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                0.25420          0.2929                  0.09873       0  \n",
       "1                0.12520          0.3415                  0.09740       0  \n",
       "2                0.20480          0.3679                  0.09870       0  \n",
       "3                0.25430          0.3109                  0.09061       0  \n",
       "4                0.19410          0.2818                  0.10050       0  \n",
       "5                0.12250          0.3020                  0.06846       0  \n",
       "6                0.24620          0.3277                  0.10190       0  \n",
       "7                0.09975          0.2948                  0.08452       0  \n",
       "8                0.22580          0.2807                  0.10710       0  \n",
       "9                0.15650          0.2718                  0.09353       0  \n",
       "10               0.05013          0.1987                  0.06169       1  \n",
       "11               0.06517          0.2878                  0.09211       1  \n",
       "12               0.08235          0.3024                  0.06949       1  \n",
       "13               0.09310          0.3020                  0.09646       1  \n",
       "14               0.07431          0.2694                  0.06878       1  \n",
       "15               0.06913          0.2535                  0.07993       1  \n",
       "16               0.06608          0.3207                  0.07247       1  \n",
       "17               0.09749          0.2622                  0.08490       1  \n",
       "18               0.06754          0.3202                  0.07287       1  \n",
       "19               0.07887          0.2251                  0.07732       1  \n",
       "\n",
       "[20 rows x 31 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function!\n",
    "stratified_breast_cancer = stratified_sampling(breast_cancer, 10, 'target')\n",
    "\n",
    "# View your output\n",
    "stratified_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at distribution of target values, they supposed to be equally sampled.\n",
    "plt.hist(stratified_breast_cancer[\"target\"])\n",
    "plt.show()\n",
    "assert(stratified_breast_cancer.shape[0] == 20)\n",
    "assert(sum(stratified_breast_cancer.target == 0) == 10)\n",
    "assert(sum(stratified_breast_cancer.target == 1) == 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-95e3fb2c7191077b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3. Decision Trees [Vodelina Samatova]\n",
    "\n",
    "Now we are going to classify the malignant versus benign cases (the zeros versus the ones) with Decision Trees.\n",
    "\n",
    "You can perform classification using a DecisionTree in python using the scikit-learn library. \n",
    "\n",
    "Take a look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) to get a clear understanding of all function arguments.\n",
    "\n",
    "Given below is a simple toy example for you to learn how to use the DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86cae1f3ae8215f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "# we will use the MinMaxScaler method to scale our data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "# y: target\n",
    "# Check the shape of y\n",
    "x=breast_cancer.drop(columns=['target']) #All columns except 'target'\n",
    "y=breast_cancer['target'] #'target' column\n",
    "\n",
    "#shape of y\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n"
     ]
    }
   ],
   "source": [
    "# X: predictors\n",
    "# Let's drop the column for the target variable\n",
    "X = breast_cancer.drop(columns=['target'])\n",
    "# Check the shape of X\n",
    "print(X.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-08e1947248aafe00",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the first step, we will split our data into train and test subsets. To get more insight into the function we are using here (*train_test_split()*), take a look at [this tutorial.](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "* `X_train` is the features (independent variables) of the training dataset.\n",
    "* `y_train` is the target (dependent) variable of the training dataset.\n",
    "* `X_test` is the features (independent variables) of the test dataset.\n",
    "* `y_test` is the target (dependent) variable of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(X, y, \n",
    "                     test_size=0.2, \n",
    "                     validate_size=0.2, \n",
    "                     random_state=0):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, test_size=test_size, random_state = random_state)\n",
    "\n",
    "    # We need to calculate a new split size (the proportion of validation to the remaining)\n",
    "    \n",
    "    # let's assume we had 100 samples and we don't do this\n",
    "    # then the split will be 20 + (20% of 80) + (80% of 80). \n",
    "    # But we want 20 + 20 + 60\n",
    "    new_validate_size = validate_size / (1 - test_size)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, stratify=y_train, \n",
    "        test_size=new_validate_size, \n",
    "        random_state = random_state)\n",
    "\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0d6ef4b8b55ebc23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Split data into training, validation, and testing data sets\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = stratified_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (X_train and y_train): \t (341, 30)  \t (341,)\n",
      "Validation (X_val and y_val): \t\t (114, 30)  \t (114,)\n",
      "Testing (X_test and y_test): \t\t (114, 30)   \t (114,)\n"
     ]
    }
   ],
   "source": [
    "# Examine the split proportions \n",
    "\n",
    "print (\"Training (X_train and y_train): \\t\", X_train.shape, \" \\t\", y_train.shape)\n",
    "\n",
    "print (\"Validation (X_val and y_val): \\t\\t\", X_val.shape, \" \\t\", y_val.shape)\n",
    "\n",
    "print (\"Testing (X_test and y_test): \\t\\t\", X_test.shape, \"  \\t\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>13.660</td>\n",
       "      <td>15.15</td>\n",
       "      <td>88.27</td>\n",
       "      <td>580.6</td>\n",
       "      <td>0.08268</td>\n",
       "      <td>0.07548</td>\n",
       "      <td>0.04249</td>\n",
       "      <td>0.02471</td>\n",
       "      <td>0.1792</td>\n",
       "      <td>0.05897</td>\n",
       "      <td>...</td>\n",
       "      <td>14.540</td>\n",
       "      <td>19.64</td>\n",
       "      <td>97.96</td>\n",
       "      <td>657.0</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.3104</td>\n",
       "      <td>0.25690</td>\n",
       "      <td>0.10540</td>\n",
       "      <td>0.3387</td>\n",
       "      <td>0.09638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>9.295</td>\n",
       "      <td>13.90</td>\n",
       "      <td>59.96</td>\n",
       "      <td>257.8</td>\n",
       "      <td>0.13710</td>\n",
       "      <td>0.12250</td>\n",
       "      <td>0.03332</td>\n",
       "      <td>0.02421</td>\n",
       "      <td>0.2197</td>\n",
       "      <td>0.07696</td>\n",
       "      <td>...</td>\n",
       "      <td>10.570</td>\n",
       "      <td>17.84</td>\n",
       "      <td>67.84</td>\n",
       "      <td>326.6</td>\n",
       "      <td>0.18500</td>\n",
       "      <td>0.2097</td>\n",
       "      <td>0.09996</td>\n",
       "      <td>0.07262</td>\n",
       "      <td>0.3681</td>\n",
       "      <td>0.08982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>8.888</td>\n",
       "      <td>14.64</td>\n",
       "      <td>58.79</td>\n",
       "      <td>244.0</td>\n",
       "      <td>0.09783</td>\n",
       "      <td>0.15310</td>\n",
       "      <td>0.08606</td>\n",
       "      <td>0.02872</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>0.08980</td>\n",
       "      <td>...</td>\n",
       "      <td>9.733</td>\n",
       "      <td>15.67</td>\n",
       "      <td>62.56</td>\n",
       "      <td>284.4</td>\n",
       "      <td>0.12070</td>\n",
       "      <td>0.2436</td>\n",
       "      <td>0.14340</td>\n",
       "      <td>0.04786</td>\n",
       "      <td>0.2254</td>\n",
       "      <td>0.10840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>14.270</td>\n",
       "      <td>22.55</td>\n",
       "      <td>93.77</td>\n",
       "      <td>629.8</td>\n",
       "      <td>0.10380</td>\n",
       "      <td>0.11540</td>\n",
       "      <td>0.14630</td>\n",
       "      <td>0.06139</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.05982</td>\n",
       "      <td>...</td>\n",
       "      <td>15.290</td>\n",
       "      <td>34.27</td>\n",
       "      <td>104.30</td>\n",
       "      <td>728.3</td>\n",
       "      <td>0.13800</td>\n",
       "      <td>0.2733</td>\n",
       "      <td>0.42340</td>\n",
       "      <td>0.13620</td>\n",
       "      <td>0.2698</td>\n",
       "      <td>0.08351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>12.800</td>\n",
       "      <td>17.46</td>\n",
       "      <td>83.05</td>\n",
       "      <td>508.3</td>\n",
       "      <td>0.08044</td>\n",
       "      <td>0.08895</td>\n",
       "      <td>0.07390</td>\n",
       "      <td>0.04083</td>\n",
       "      <td>0.1574</td>\n",
       "      <td>0.05750</td>\n",
       "      <td>...</td>\n",
       "      <td>13.740</td>\n",
       "      <td>21.06</td>\n",
       "      <td>90.72</td>\n",
       "      <td>591.0</td>\n",
       "      <td>0.09534</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.19010</td>\n",
       "      <td>0.08296</td>\n",
       "      <td>0.1988</td>\n",
       "      <td>0.07053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>12.250</td>\n",
       "      <td>22.44</td>\n",
       "      <td>78.18</td>\n",
       "      <td>466.5</td>\n",
       "      <td>0.08192</td>\n",
       "      <td>0.05200</td>\n",
       "      <td>0.01714</td>\n",
       "      <td>0.01261</td>\n",
       "      <td>0.1544</td>\n",
       "      <td>0.05976</td>\n",
       "      <td>...</td>\n",
       "      <td>14.170</td>\n",
       "      <td>31.99</td>\n",
       "      <td>92.74</td>\n",
       "      <td>622.9</td>\n",
       "      <td>0.12560</td>\n",
       "      <td>0.1804</td>\n",
       "      <td>0.12300</td>\n",
       "      <td>0.06335</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.08203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>13.240</td>\n",
       "      <td>20.13</td>\n",
       "      <td>86.87</td>\n",
       "      <td>542.9</td>\n",
       "      <td>0.08284</td>\n",
       "      <td>0.12230</td>\n",
       "      <td>0.10100</td>\n",
       "      <td>0.02833</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.06432</td>\n",
       "      <td>...</td>\n",
       "      <td>15.440</td>\n",
       "      <td>25.50</td>\n",
       "      <td>115.00</td>\n",
       "      <td>733.5</td>\n",
       "      <td>0.12010</td>\n",
       "      <td>0.5646</td>\n",
       "      <td>0.65560</td>\n",
       "      <td>0.13570</td>\n",
       "      <td>0.2845</td>\n",
       "      <td>0.12490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>12.470</td>\n",
       "      <td>18.60</td>\n",
       "      <td>81.09</td>\n",
       "      <td>481.9</td>\n",
       "      <td>0.09965</td>\n",
       "      <td>0.10580</td>\n",
       "      <td>0.08005</td>\n",
       "      <td>0.03821</td>\n",
       "      <td>0.1925</td>\n",
       "      <td>0.06373</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970</td>\n",
       "      <td>24.64</td>\n",
       "      <td>96.05</td>\n",
       "      <td>677.9</td>\n",
       "      <td>0.14260</td>\n",
       "      <td>0.2378</td>\n",
       "      <td>0.26710</td>\n",
       "      <td>0.10150</td>\n",
       "      <td>0.3014</td>\n",
       "      <td>0.08750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>13.000</td>\n",
       "      <td>25.13</td>\n",
       "      <td>82.61</td>\n",
       "      <td>520.2</td>\n",
       "      <td>0.08369</td>\n",
       "      <td>0.05073</td>\n",
       "      <td>0.01206</td>\n",
       "      <td>0.01762</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.05449</td>\n",
       "      <td>...</td>\n",
       "      <td>14.340</td>\n",
       "      <td>31.88</td>\n",
       "      <td>91.06</td>\n",
       "      <td>628.5</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.1093</td>\n",
       "      <td>0.04462</td>\n",
       "      <td>0.05921</td>\n",
       "      <td>0.2306</td>\n",
       "      <td>0.06291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.450</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>15.470</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.53550</td>\n",
       "      <td>0.17410</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>341 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "378       13.660         15.15           88.27      580.6          0.08268   \n",
       "520        9.295         13.90           59.96      257.8          0.13710   \n",
       "71         8.888         14.64           58.79      244.0          0.09783   \n",
       "536       14.270         22.55           93.77      629.8          0.10380   \n",
       "397       12.800         17.46           83.05      508.3          0.08044   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "490       12.250         22.44           78.18      466.5          0.08192   \n",
       "465       13.240         20.13           86.87      542.9          0.08284   \n",
       "204       12.470         18.60           81.09      481.9          0.09965   \n",
       "458       13.000         25.13           82.61      520.2          0.08369   \n",
       "5         12.450         15.70           82.57      477.1          0.12780   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "378           0.07548         0.04249              0.02471         0.1792   \n",
       "520           0.12250         0.03332              0.02421         0.2197   \n",
       "71            0.15310         0.08606              0.02872         0.1902   \n",
       "536           0.11540         0.14630              0.06139         0.1926   \n",
       "397           0.08895         0.07390              0.04083         0.1574   \n",
       "..                ...             ...                  ...            ...   \n",
       "490           0.05200         0.01714              0.01261         0.1544   \n",
       "465           0.12230         0.10100              0.02833         0.1601   \n",
       "204           0.10580         0.08005              0.03821         0.1925   \n",
       "458           0.05073         0.01206              0.01762         0.1667   \n",
       "5             0.17000         0.15780              0.08089         0.2087   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "378                 0.05897  ...        14.540          19.64   \n",
       "520                 0.07696  ...        10.570          17.84   \n",
       "71                  0.08980  ...         9.733          15.67   \n",
       "536                 0.05982  ...        15.290          34.27   \n",
       "397                 0.05750  ...        13.740          21.06   \n",
       "..                      ...  ...           ...            ...   \n",
       "490                 0.05976  ...        14.170          31.99   \n",
       "465                 0.06432  ...        15.440          25.50   \n",
       "204                 0.06373  ...        14.970          24.64   \n",
       "458                 0.05449  ...        14.340          31.88   \n",
       "5                   0.07613  ...        15.470          23.75   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "378            97.96       657.0           0.12750             0.3104   \n",
       "520            67.84       326.6           0.18500             0.2097   \n",
       "71             62.56       284.4           0.12070             0.2436   \n",
       "536           104.30       728.3           0.13800             0.2733   \n",
       "397            90.72       591.0           0.09534             0.1812   \n",
       "..               ...         ...               ...                ...   \n",
       "490            92.74       622.9           0.12560             0.1804   \n",
       "465           115.00       733.5           0.12010             0.5646   \n",
       "204            96.05       677.9           0.14260             0.2378   \n",
       "458            91.06       628.5           0.12180             0.1093   \n",
       "5             103.40       741.6           0.17910             0.5249   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "378          0.25690               0.10540          0.3387   \n",
       "520          0.09996               0.07262          0.3681   \n",
       "71           0.14340               0.04786          0.2254   \n",
       "536          0.42340               0.13620          0.2698   \n",
       "397          0.19010               0.08296          0.1988   \n",
       "..               ...                   ...             ...   \n",
       "490          0.12300               0.06335          0.3100   \n",
       "465          0.65560               0.13570          0.2845   \n",
       "204          0.26710               0.10150          0.3014   \n",
       "458          0.04462               0.05921          0.2306   \n",
       "5            0.53550               0.17410          0.3985   \n",
       "\n",
       "     worst fractal dimension  \n",
       "378                  0.09638  \n",
       "520                  0.08982  \n",
       "71                   0.10840  \n",
       "536                  0.08351  \n",
       "397                  0.07053  \n",
       "..                       ...  \n",
       "490                  0.08203  \n",
       "465                  0.12490  \n",
       "204                  0.08750  \n",
       "458                  0.06291  \n",
       "5                    0.12440  \n",
       "\n",
       "[341 rows x 30 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect X_train\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378    1\n",
       "520    1\n",
       "71     1\n",
       "536    0\n",
       "397    1\n",
       "      ..\n",
       "490    1\n",
       "465    1\n",
       "204    1\n",
       "458    1\n",
       "5      0\n",
       "Name: target, Length: 341, dtype: int32"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect y_train\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3ca4dd1d21003d81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now implement a decision tree function below to do the following:\n",
    "\n",
    "1. Train a decision tree model with the **training datasets** created above (using **gini index** as the criteria and **random_state = 1234** in order to keep consistency).\n",
    "2. Use the model trained from step 1 to make predictions on the **test data** created above\n",
    "\n",
    "**Store the predictions in the variable `dtree_predictions`**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "dtree",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the dtree function\n",
    "def dtree(X_train, y_train, X_test, criterion='gini', random_state=1234):\n",
    "    # Create the decision tree classifier\n",
    "    dtree_model = DecisionTreeClassifier(criterion=criterion, random_state=random_state)\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    dtree_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    predictions = dtree_model.predict(X_test)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 1 1\n",
      " 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1\n",
      " 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Test your result!\n",
    "dtree_predictions = dtree(X_train, y_train, X_test, 'gini', 1234)\n",
    "\n",
    "print(dtree_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c87ab101a0fff1b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this step, you will look into the accuracy score of the decision tree predictions. In other words, you would compare the predictions of the decision tree with the actual test labels you have in the testing set. \n",
    "\n",
    "For more documentation, take a look at [this article.](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9035087719298246"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(dtree_predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "dtree-public",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Public tests for dtree\n",
    "np.testing.assert_almost_equal(\n",
    "    accuracy_score(dtree_predictions,\n",
    "                   y_test),\n",
    "    0.9035087719298246\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f2cfa4389baa6ce4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4. Feature Selection [Vodelina Samatova]\n",
    "\n",
    "In this problem, you will see if feature selection can improve the accuracy of your classifier!\n",
    "\n",
    "We do feature selection to remove unnecessary features, and also to see which features are most useful for prediction. You could imagine predicting diabetes progression would be easier if we know which features are relevant for doing so.\n",
    "\n",
    "In the function below, you will implement a feature selection aglorithm, which selects the $k$ best features according to some measure. In this case we'll use the  [ANOVA F-value](https://www.statisticshowto.com/probability-and-statistics/f-statistic-value-test/), which is a measure of the linear relationship between each feature and the target variable (just like correlation).\n",
    "\n",
    "**Note**: Remember, we can't peek at the test data, even during feature selection (if we knew what features were useful on the test dataset, that would be unrealistic). Therefore, it is important that you fit the the feature selction function **ONLY** on the *training* set. One the feature selector is fit (i.e. figures out which features to keep), you can use it to transform both training and testing dataset (`X_train` and `X_test`), i.e. remove the unneeded features.\n",
    "\n",
    "**HINT**: Feature selection is performed using [SelectKBest function](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) with [ANOVA F-value](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif) function.\n",
    "\n",
    "**HINT**: In python you can return multiple values, separated by commas, e.g. `return 1, 2, 3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "feature_selection",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "def feature_selection(X_train, y_train, X_test, k=7):\n",
    "    \"\"\"\n",
    "     Input:\n",
    "          x_train: A numpy array of shape (n_training_rows, n_attributes) where n_training_rows refers to \n",
    "          the number of rows in your training dataset and n_attributes refers to the number of attributes. \n",
    "          y_train: A numpy array of shape (n_training_rows, ) containing the class labels for each row in your \n",
    "          training dataset.\n",
    "          x_test: A numpy array of shape (n_testing_rows, n_attributes) where n_testing_rows refers to the number \n",
    "          of rows in your testing dataset and n_attributes refers to the number of attributes. \n",
    "          k: number of features to select.\n",
    "    Output:\n",
    "          fs: The fit feature selector\n",
    "          X_train_selected: The transformed training set, with features selected. \n",
    "                            The result should be a numpy array of shape (n_training_rows, k).\n",
    "          X_test_selected: The transformed testing set, with features selected.\n",
    "    \n",
    "    Allowed Libraries: sklearn\n",
    "    \"\"\"\n",
    "    # define feature selection\n",
    "    fs = SelectKBest(score_func=f_classif, k=k)\n",
    "\n",
    "    #Fit the feature selector on the training data\n",
    "    X_train_selected = fs.fit_transform(X_train, y_train)\n",
    "\n",
    "    #Transform the test data based on the fitted selector\n",
    "    X_test_selected = fs.transform(X_test)\n",
    "\n",
    "    return fs, X_train_selected, X_test_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.88025770e+02 7.08854777e+01 4.18029021e+02 3.36880602e+02\n",
      " 5.01255702e+01 2.05381888e+02 3.95714153e+02 5.53613324e+02\n",
      " 4.34502810e+01 3.52685992e-02 1.84879323e+02 3.68284617e-01\n",
      " 1.77655322e+02 1.60615071e+02 4.51961967e-01 4.28794755e+01\n",
      " 3.85754122e+01 1.01944172e+02 1.46946004e-01 8.18170914e+00\n",
      " 4.79579810e+02 9.18694127e+01 4.95696897e+02 3.52940911e+02\n",
      " 6.96036727e+01 1.84940066e+02 2.81178021e+02 5.70465714e+02\n",
      " 6.35428353e+01 4.64644363e+01]\n",
      "(341, 7)\n",
      "(114, 7)\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "fs,X_train_selected,X_test_selected = feature_selection(X_train, y_train, X_test, 7)\n",
    "\n",
    "# Check the score for each individual feature (i.e. how important is each feature)\n",
    "print(fs.scores_)\n",
    "\n",
    "# Check whether acheived seven best attributes for both training and testing set\n",
    "print(X_train_selected.shape)\n",
    "print(X_test_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features are: \n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "Selected best feautures are: \n",
      "['mean radius' 'mean perimeter' 'mean concavity' 'mean concave points'\n",
      " 'worst radius' 'worst perimeter' 'worst concave points']\n"
     ]
    }
   ],
   "source": [
    "# Let's get the name of the selected features\n",
    "\n",
    "print(\"Original features are: \\n\" + str(X_train.columns.values))\n",
    "\n",
    "print(\"Selected best feautures are: \\n\" + str(X_train.columns.values[fs.get_support()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "feature_selection_public",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Public tests for feature_selection\n",
    "fs, selected_train, selected_test = feature_selection(X_train, y_train, X_test, 5)\n",
    "assert (selected_train.shape[1] == 5)\n",
    "assert (selected_test.shape[1] == 5)\n",
    "assert ('concavity error' not in X_train.columns.values[fs.get_support()])\n",
    "assert ('worst radius' in X_train.columns.values[fs.get_support()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-980493e5987405de2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 5. Feature Transformation [Sogolsadat Mansouri]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will combine feature selection with normalization into a *pipeline*, and test whether it improves our mode.\n",
    "\n",
    "In some cases, you would want to perform certain feature transformations such as z-score normalization.\n",
    "\n",
    "Take a look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to get a clear understanding of all function argumetns for z-score normalization.\n",
    "\n",
    "Given below is a simple example for you to learn how to use z-score normalization and combine it with other data processing procedures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use a toy dataset to demonstrate z-score normalization. \n",
    "# We are using the wine dataset from sklearn.datasets as our toy dataset.\n",
    "\n",
    "# First load the data\n",
    "toy_dataset = datasets.load_wine(as_frame=True).data\n",
    "\n",
    "# To emulate an actual model learning process, we split the data set into trainining and testing set\n",
    "toy_train, toy_test = train_test_split(toy_dataset, test_size = 0.33)\n",
    "\n",
    "# Here we use StandardScaler class from sklearn to normalize\n",
    "toy_norm_train = StandardScaler().fit(toy_train).transform(toy_train)\n",
    "\n",
    "# Notice here we use the training set to fit the StandardScaler, i.e. compute the mean and standard deviation,\n",
    "# and use it to normalize testing set, just like with feature selection, above.\n",
    "# This is because in real-world deployment case, you very likely won't know the parameter of unseen data distribution\n",
    "# (testing set). So a normal approach is to use distribution parameters estimated from training set to transform\n",
    "# unseen data.\n",
    "toy_norm_test = StandardScaler().fit(toy_train).transform(toy_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alcohol                          12.999076\n",
       "malic_acid                        2.357395\n",
       "ash                               2.372437\n",
       "alcalinity_of_ash                19.660504\n",
       "magnesium                        99.159664\n",
       "total_phenols                     2.222101\n",
       "flavanoids                        1.955630\n",
       "nonflavanoid_phenols              0.364202\n",
       "proanthocyanins                   1.601849\n",
       "color_intensity                   5.046639\n",
       "hue                               0.943328\n",
       "od280/od315_of_diluted_wines      2.570168\n",
       "proline                         734.478992\n",
       "dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the means are all different\n",
    "toy_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alcohol                           0.831553\n",
       "malic_acid                        1.133244\n",
       "ash                               0.238750\n",
       "alcalinity_of_ash                 3.265127\n",
       "magnesium                        14.363314\n",
       "total_phenols                     0.645206\n",
       "flavanoids                        0.949978\n",
       "nonflavanoid_phenols              0.127029\n",
       "proanthocyanins                   0.590017\n",
       "color_intensity                   2.352255\n",
       "hue                               0.218627\n",
       "od280/od315_of_diluted_wines      0.718058\n",
       "proline                         305.101646\n",
       "dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And the standard deviations vary\n",
    "toy_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.966680786478849e-15\n",
      "6.55871249001248e-16\n",
      "-3.4300293697767965e-15\n",
      "1.8211389445952146e-15\n"
     ]
    }
   ],
   "source": [
    "# Now column means are all near 0\n",
    "print(toy_norm_train[:,0].mean())\n",
    "print(toy_norm_train[:,1].mean())\n",
    "print(toy_norm_train[:,2].mean())\n",
    "print(toy_norm_train[:,3].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0000000000000002\n",
      "0.9999999999999999\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Now and standard deviations are near 1\n",
    "print(toy_norm_train[:,0].std())\n",
    "print(toy_norm_train[:,1].std())\n",
    "print(toy_norm_train[:,2].std())\n",
    "print(toy_norm_train[:,3].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2d0dab0264a912db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You will build a pipeline that perform both feature selection (using your earlier function) and z-score normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "normalize_feature_select",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def normalize_feature_select(X_train, y_train, X_test, k = 7):\n",
    "    \"\"\"\n",
    "    You will build a pipeline that perform the following steps:\n",
    "        1. z-score normalize the x_train and x_test using x_train.\n",
    "        2. perform feature selection.\n",
    "    \n",
    "    Your inputs and outputs are as shown below:\n",
    "    \n",
    "    Input:\n",
    "          x_train: A numpy array of shape (n_training_rows, n_attributes) where n_training_rows refers to \n",
    "              the number of rows in your training dataset and n_attributes refers to the number of attributes. \n",
    "          y_train: A numpy array of shape (n_training_rows, ) containing the class labels for each row in your \n",
    "              training dataset.\n",
    "          x_test: A numpy array of shape (n_test_rows, n_attributes) where n_test_rows refers to the number \n",
    "              of rows in your target dataset and n_attributes refers to the number of attributes. \n",
    "          k: number of features to select.\n",
    "    Output:\n",
    "          x_train_selected: A numpy array of shape (n_train_rows, n_selected_attributes) containing \n",
    "              z-score normalized data from x_train with selected features only. \n",
    "              n_selected_attributes is the number of selected features.\n",
    "          x_test_selected: A numpy array of shape (n_test_rows, n_selected_attributes) containing \n",
    "              z-score normalized data from x_test with selected features only. \n",
    "              n_selected_attributes is the number of selected features.\n",
    "          \n",
    "    Allowed Libraries: sklearn\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_norm_train = scaler.transform(X_train)\n",
    "    X_norm_test = scaler.transform(X_test)\n",
    "    \n",
    "    _, X_train_selected, X_test_selected = feature_selection(X_norm_train, y_train, X_norm_test, k=k)\n",
    "    \n",
    "    return X_train_selected, X_test_selected\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.521813049162069e-16\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "#Test your code!\n",
    "X_norm_selected_train, X_norm_selected_test = normalize_feature_select(X_train, y_train, X_test, k = 7)\n",
    "\n",
    "# Mean of attribute 0 should be 0\n",
    "print(X_norm_selected_train[:,0].mean())\n",
    "# Standard deviation of attriute 0 should be 1\n",
    "print(X_norm_selected_train[:,0].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "normalize_feature_select-public",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Public tests for feature_selection\n",
    "selected_train, selected_test = normalize_feature_select(X_train, y_train, X_test, 5)\n",
    "assert (selected_train.shape[1] == 5)\n",
    "assert (selected_test.shape[1] == 5)\n",
    "for i in range(0, 5):\n",
    "    np.testing.assert_almost_equal(X_norm_selected_train[:,i].mean(), 0)\n",
    "    np.testing.assert_almost_equal(X_norm_selected_train[:,i].std(), 1)\n",
    "    # test set shouldn't be perfectly centered or scaled\n",
    "    assert X_norm_selected_test[:,i].mean() != 0\n",
    "    assert X_norm_selected_test[:,i].std() != 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b77e84f5052da690",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now is the moment of truth - did the decision tree improve our classifier? Let's compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9035087719298246"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's recreate the original decision tree.\n",
    "dtree_predictions = dtree(X_train, y_train, X_test, 'gini', 1234) # Random see for consistency\n",
    "accuracy_score(dtree_predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9122807017543859"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next the normalized, feature selected tree\n",
    "dtree_norm_selected_predictions = dtree(X_norm_selected_train, y_train, X_norm_selected_test, 'gini', 1234)\n",
    "accuracy_score(dtree_norm_selected_predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0d8760508efb3d84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Which features did better - the originals or the normalized, feature selected ones? How much of a difference did preprocessing make? Based on what you know about decision trees, z-score normalization and feature selection, why do you think this was the case? Answer below in 2-3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "comparing-trees",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "### Answer\n",
    "\n",
    "The tree which was trained on normalized and feature selected data performs slightly better than the original decision tree in terms of accuracy. The preprocessing made a difference of 0.0087 in the accuracy of normalized and normal tree. According to me, feature selection has affected the performace slightly as it discarded the irrelevant features and improved the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-980493e5987405de23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 6. PCA [Sogolsadat Mansouri]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c1694bc4582d3eed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Examples\n",
    "\n",
    "You can perform PCA in python using the scikit-learn library. Take a look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to get a clear understanding of all function arguments.\n",
    "\n",
    "Given below is a simple toy example for you to learn how to use PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-def830bb53f756ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# let's use a toy dataset to demonstrate PCA. We are using the wine dataset from sklearn.datasets as our toy dataset.\n",
    "# We will apply PCA on it and extract the first two principal components. \n",
    "# While there are ways to directly extract the principal components using the sklearn methods, for the purpose \n",
    "# of this exercise, we will first extract the eigen vectors and then calculate the principal components from these eigen vectors. \n",
    "\n",
    "# first, load and z-score normalize the data\n",
    "toy_dataset_sk = datasets.load_wine(as_frame=True)\n",
    "toy_dataset = pd.DataFrame(StandardScaler().fit_transform(toy_dataset_sk.data), columns = toy_dataset_sk.feature_names)\n",
    "\n",
    "# Display the dataset for your reference\n",
    "# note that you can use the display() method to display your pandas dataframe in Jupyter\n",
    "#display(toy_dataset)\n",
    "\n",
    "# apply PCA on the toy dataset and extract the eigen vectors of the first two principal components\n",
    "toy_pca = PCA(n_components = 2).fit(toy_dataset)\n",
    "toy_eigen_vectors = toy_pca.components_\n",
    "\n",
    "# now extract the first two principal components\n",
    "# Recall from class material how to do this.  \n",
    "# Take a look at matrix multiplication using numpy here: \n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.matmul.html\n",
    "toy_principal_components = np.matmul(toy_dataset.values , toy_eigen_vectors.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 2)\n",
      "(178, 13)\n"
     ]
    }
   ],
   "source": [
    "# Explore the outputs of the eigen vectors and principal components to gain a better understanding\n",
    "# Explore other outputs...\n",
    "print(toy_eigen_vectors.T.shape)\n",
    "print(toy_dataset.values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ba4a2707304846b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 6.1: Extracting Eigen Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "principal_component_analysis",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Problem 1\n",
    "def principal_component_analysis(data, n_components):\n",
    "      return PCA(n_components = n_components).fit(data).components_\n",
    "\n",
    "   \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.99525297e-03, 2.14514374e-03, 3.45336870e-02, 5.14453612e-01,\n",
       "       3.88827136e-06, 4.03315839e-05, 8.18903314e-05, 4.75569636e-05])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "principal_component_analysis(X_train, 2)[0, :8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 30)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function!\n",
    "# note that you can convert your pandas data frame into a numpy matrix by using the dataframe.values property\n",
    "eigen_vectors = principal_component_analysis(X_train, 2)\n",
    "# Each of the 2 eigen vectors should have a value/weight for each of the original 13 attributes\n",
    "eigen_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "principal_component_analysis-public",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert principal_component_analysis(X_train, 2).shape == (2, 30)\n",
    "np.testing.assert_almost_equal(\n",
    "    principal_component_analysis(X_train, 2)[0, :8], \n",
    "    np.array(\n",
    "        [4.99525297e-03, 2.14514374e-03, 3.45336870e-02, 5.14453612e-01,\n",
    "       3.88827136e-06, 4.03315839e-05, 8.18903314e-05, 4.75569636e-05]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b8065cf6aaf5b93c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 6.2. Calculating Principal Components from Eigen Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "principal_component_calculation",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Problem 2\n",
    "def principal_component_calculation(data, component_weights):\n",
    "      return np.matmul(data, component_weights.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal components are =               0           1\n",
      "378  868.291103 -164.411521\n",
      "520  418.093546  -55.437415\n",
      "71   374.966714  -64.673713\n",
      "536  955.426179 -169.519136\n",
      "397  775.242686 -134.497198\n",
      "..          ...         ...\n",
      "490  780.278195  -83.161657\n",
      "465  915.691687  -92.041880\n",
      "204  836.047126  -67.430863\n",
      "458  812.934184 -126.096073\n",
      "5    888.198708  -31.182813\n",
      "\n",
      "[341 rows x 2 columns] and their shape is (341, 2)\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "# note that eigen_vectors are the eigen vectors you calculated earlier using principal_component_analysis. \n",
    "# We're calculating it again here for ease of use\n",
    "eigen_vectors = principal_component_analysis(X_train, 2)\n",
    "principal_component_values = principal_component_calculation(X_train, eigen_vectors)\n",
    "print(f'Principal components are = {principal_component_values} and their shape is {principal_component_values.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "principal_component_calculation-public",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_eigen_vectors = principal_component_analysis(X_train, 2)\n",
    "test_pc = principal_component_calculation(X_train, test_eigen_vectors)\n",
    "assert test_pc.shape == (X_train.shape[0], 2)\n",
    "np.testing.assert_almost_equal(test_pc.iloc[0, :].values, [868.291104, -164.411521], decimal=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b1eb7d2a4006c6a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 6.3: Visualize your results\n",
    "\n",
    "You've gained familiarity with matplotlib in HW0. Now use matplotlib to\n",
    "\n",
    "a) generate a plot with the first principal component on x-axis, second principal component on y-axis. \n",
    "\n",
    "b) Assign color to each data point according to the target value. You can do this using the splitted y_train or y_test, depending on which set you are plotting.  \n",
    "\n",
    "***Hint 1:*** Take a look at the [plt.scatter](https://matplotlib.org/3.3.3/api/_as_gen/matplotlib.pyplot.scatter.html) function. Pay close attention to the 'c' variable. \n",
    "\n",
    "c) Name the x-axis as \"PC1\", y-axis as \"PC2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is an example of a scatter plot, using the bmi and age of participants\n",
    "plt.scatter(X_train.iloc[:, 0], X_train.iloc[:, 2], c=y_train)\n",
    "plt.xlabel(X_train.columns[0])\n",
    "plt.ylabel(X_train.columns[2])\n",
    "plt.show()\n",
    "\n",
    "# How well does this plot separate the 2 different classes?\n",
    "    # The plot seperates the two classes pretty well however there is a bit of an overlap i.e wrongly classifed classes for some values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c684b0d6c8af3bab",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Problem 3 (see question above)\n",
    "\n",
    "# recalculating eigen vectors and principal components for ease of use\n",
    "eigen_vectors = principal_component_analysis(X_train, 2)\n",
    "principal_component_values = principal_component_calculation(X_train, eigen_vectors)\n",
    "\n",
    "plt.scatter(principal_component_values.iloc[:, 0], principal_component_values.iloc[:, 1], c=y_train)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the first 2 PCs separate the data better than the first 2 numeric attributes?\n",
    "\n",
    "### Answer\n",
    "\n",
    "No, the first 2 PCs do not perform better in separating the data than the first 2 numerica attributes. As can be observed, in the first 2 numeric attributes, the purple and yellow points are clustered closely in separate regions (purple in the upper region, yellow in the lower region). On the other hand, in the 2 PCs, the purple points are scattered widely, somewhat overlapping with the yellow points."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
